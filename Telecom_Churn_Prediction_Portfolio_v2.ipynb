{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8e96b5",
   "metadata": {},
   "source": [
    "# Proyecto de Churn en Telecomunicaciones: Análisis y Modelado Predictivo\n",
    "\n",
    "**Objetivo del Proyecto:**\n",
    "\n",
    "Este proyecto tiene como objetivo analizar los datos de una empresa de telecomunicaciones para construir un modelo de Machine Learning capaz de predecir qué clientes tienen una alta probabilidad de cancelar su servicio (churn). La identificación temprana de estos clientes permite a la empresa tomar acciones proactivas para retenerlos, reduciendo así la pérdida de ingresos.\n",
    "\n",
    "**Metodología:**\n",
    "\n",
    "El proyecto sigue un flujo de trabajo estructurado de ciencia de datos:\n",
    "1.  **Carga y Exploración de Datos:** Se cargarán y examinarán los diferentes conjuntos de datos para comprender su estructura y contenido.\n",
    "2.  **Preprocesamiento y Limpieza:** Se realizarán tareas de limpieza, como la conversión de tipos de datos, el manejo de valores ausentes y la unificación de los datos en un solo DataFrame.\n",
    "3.  **Análisis Exploratorio de Datos (EDA):** Se analizarán las variables para descubrir patrones, correlaciones y características que influyen en el churn.\n",
    "4.  **Preparación de Datos para Modelado:** Se prepararán los datos para el entrenamiento de modelos, incluyendo la codificación de variables categóricas y el escalado de características numéricas.\n",
    "5.  **Entrenamiento y Evaluación de Modelos:** Se entrenarán y evaluarán varios modelos de clasificación para identificar el de mejor rendimiento en la predicción del churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75531329",
   "metadata": {},
   "source": [
    "## 1. Carga y Exploración Inicial de Datos\n",
    "\n",
    "En esta primera fase, cargamos todas las librerías necesarias y los archivos de datos. Cada archivo (`contract.csv`, `internet.csv`, `personal.csv`, `phone.csv`) contiene una pieza diferente de la información del cliente. Realizaremos una inspección inicial para entender la estructura, los tipos de datos y la presencia de valores nulos en cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a5c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 1: IMPORTACIÓN DE LIBRERÍAS\n",
    "# ============================================================================================\n",
    "# Se importan las librerías fundamentales para el análisis y la manipulación de datos.\n",
    "\n",
    "# Pandas: para la manipulación y análisis de datos tabulares (DataFrames).\n",
    "import pandas as pd\n",
    "# NumPy: para operaciones numéricas eficientes, especialmente con arrays.\n",
    "import numpy as np\n",
    "# Matplotlib.pyplot: para la creación de visualizaciones estáticas.\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn: para crear visualizaciones estadísticas más atractivas y complejas.\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92564b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 2: CARGA DE LOS ARCHIVOS DE DATOS\n",
    "# ============================================================================================\n",
    "# Se cargan los cuatro archivos CSV que contienen los datos de los clientes.\n",
    "# Cada archivo se carga en un DataFrame de Pandas independiente.\n",
    "\n",
    "# contract.csv: Información sobre el contrato del cliente (tipo, fechas, facturación).\n",
    "contract = pd.read_csv('contract.csv')\n",
    "# internet.csv: Información sobre los servicios de internet del cliente.\n",
    "internet = pd.read_csv('internet.csv')\n",
    "# personal.csv: Datos demográficos del cliente (género, edad, etc.).\n",
    "personal = pd.read_csv('personal.csv')\n",
    "# phone.csv: Información sobre los servicios telefónicos del cliente.\n",
    "phone = pd.read_csv('phone.csv')\n",
    "\n",
    "# Se imprime la forma (filas, columnas) de cada DataFrame para verificar la carga.\n",
    "print(\"Archivos cargados. Dimensiones (filas, columnas) por archivo:\")\n",
    "print(\"contract:\", contract.shape)\n",
    "print(\"internet:\", internet.shape)\n",
    "print(\"personal:\", personal.shape)\n",
    "print(\"phone:\", phone.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21801596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 3: INSPECCIÓN RÁPIDA DE LOS DATOS\n",
    "# ============================================================================================\n",
    "# Se utiliza el método .head() para mostrar las primeras 5 filas de cada DataFrame.\n",
    "# Esto proporciona una vista rápida de la estructura y el contenido de cada tabla.\n",
    "\n",
    "print(\"Primeras filas de 'contract':\")\n",
    "display(contract.head())\n",
    "\n",
    "print(\"\\nPrimeras filas de 'internet':\")\n",
    "display(internet.head())\n",
    "\n",
    "print(\"\\nPrimeras filas de 'personal':\")\n",
    "display(personal.head())\n",
    "\n",
    "print(\"\\nPrimeras filas de 'phone':\")\n",
    "display(phone.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b860609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 4: INFORMACIÓN GENERAL DE LOS DATAFRAMES\n",
    "# ============================================================================================\n",
    "# Se utiliza el método .info() para obtener un resumen técnico de cada DataFrame.\n",
    "# Esto incluye el tipo de dato de cada columna y el conteo de valores no nulos.\n",
    "# Es un paso crucial para identificar columnas con tipos de datos incorrectos o valores ausentes.\n",
    "\n",
    "print(\"Información de 'contract':\")\n",
    "contract.info()\n",
    "\n",
    "print(\"\\nInformación de 'internet':\")\n",
    "internet.info()\n",
    "\n",
    "print(\"\\nInformación de 'personal':\")\n",
    "personal.info()\n",
    "\n",
    "print(\"\\nInformación de 'phone':\")\n",
    "phone.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ba569",
   "metadata": {},
   "source": [
    "<div style=\"font-size:12px;\">\n",
    "\n",
    "<h1 style=\"color:#1E90FF;\">Interpretación de los Tipos de Datos Iniciales</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<h2 style=\"color:#8A2BE2;\">DataFrame <strong>contract (contrato)</strong></h2>\n",
    "<p>\n",
    "Las columnas de fechas (`BeginDate`, `EndDate`) y la de cargos (`TotalCharges`) están incorrectamente como `object` (texto). Necesitan ser convertidas a `datetime` y `numeric` respectivamente para poder realizar cálculos y análisis temporales. Las demás columnas categóricas también se beneficiarán de ser convertidas al tipo `category` para optimizar memoria.\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<h2 style=\"color:#FF6347;\">DataFrame <strong>internet</strong></h2>\n",
    "<p>\n",
    "Todas las columnas de servicios (ej. `OnlineSecurity`, `TechSupport`) son de tipo `object` pero representan categorías binarias ('Yes'/'No'). Convertirlas al tipo `category` es una buena práctica para el análisis y el modelado.\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<h2 style=\"color:#228B22;\">DataFrame <strong>personal</strong></h2>\n",
    "<p>\n",
    "Similar a los otros DataFrames, las columnas demográficas como `gender`, `Partner`, y `Dependents` son categóricas y deben ser tratadas como tal. `SeniorCitizen` es un entero (0/1) pero funcionalmente es una categoría, por lo que también se convertirá.\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "<h2 style=\"color:#DAA520;\">DataFrame <strong>phone</strong></h2>\n",
    "<p>\n",
    "La columna `MultipleLines` es categórica y se convertirá para mantener la consistencia en el tratamiento de datos.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cde3e",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento y Limpieza de Datos\n",
    "\n",
    "Esta etapa es fundamental para asegurar la calidad de los datos antes del análisis. Las tareas incluyen:\n",
    "- **Conversión de Tipos de Datos:** Columnas como fechas y números que están en formato de texto se convertirán a sus tipos correctos (`datetime`, `float`).\n",
    "- **Manejo de Valores Ausentes:** Se identificarán y tratarán los valores nulos. En la columna `TotalCharges`, los valores faltantes se imputarán, mientras que en `EndDate`, los valores nulos indican que el cliente sigue activo.\n",
    "- **Unificación de Datos:** Todos los DataFrames se fusionarán en uno solo usando `customerID` como clave común. Esto nos permitirá tener una vista 360° de cada cliente.\n",
    "- **Ingeniería de Características:** Se crearán nuevas columnas que serán útiles para el análisis, como `churn` (nuestra variable objetivo) y `tenure_months` (la antigüedad del cliente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 5: CONVERSIÓN DE TIPOS Y LIMPIEZA (CONTRACT)\n",
    "# ============================================================================================\n",
    "# Se corrigen los tipos de datos en el DataFrame 'contract'.\n",
    "\n",
    "# Las columnas 'BeginDate' y 'EndDate' se convierten de texto a formato de fecha (datetime).\n",
    "# En 'EndDate', el valor 'No' (que indica un cliente activo) se reemplaza por NaT (Not a Time).\n",
    "contract['BeginDate'] = pd.to_datetime(contract['BeginDate'], errors='coerce')\n",
    "contract['EndDate'] = pd.to_datetime(contract['EndDate'].replace('No', pd.NaT), errors='coerce')\n",
    "\n",
    "# La columna 'TotalCharges' contiene espacios en blanco para algunos clientes nuevos.\n",
    "# Se reemplazan estos espacios por NaN (Not a Number) y se convierte la columna a tipo numérico.\n",
    "contract['TotalCharges'] = pd.to_numeric(contract['TotalCharges'].replace(' ', np.nan), errors='coerce')\n",
    "\n",
    "# Las columnas con un número limitado de categorías se convierten al tipo 'category' para optimizar memoria.\n",
    "for col in ['Type', 'PaperlessBilling', 'PaymentMethod']:\n",
    "    if col in contract.columns:\n",
    "        contract[col] = contract[col].astype('category')\n",
    "\n",
    "print(\"Tipos de datos corregidos en 'contract':\")\n",
    "display(contract.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 6: LIMPIEZA Y TIPOS (INTERNET, PERSONAL, PHONE)\n",
    "# ============================================================================================\n",
    "# Se convierten las columnas de texto que representan categorías al tipo 'category'\n",
    "# en los DataFrames restantes para optimizar el uso de memoria y el rendimiento.\n",
    "\n",
    "# Conversión en el DataFrame 'internet'.\n",
    "internet_cols = ['InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "for col in internet_cols:\n",
    "    if col in internet.columns:\n",
    "        internet[col] = internet[col].astype('category')\n",
    "\n",
    "# Conversión en el DataFrame 'personal'.\n",
    "personal_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents']\n",
    "for col in personal_cols:\n",
    "    if col in personal.columns:\n",
    "        personal[col] = personal[col].astype('category')\n",
    "\n",
    "# Conversión en el DataFrame 'phone'.\n",
    "if 'MultipleLines' in phone.columns:\n",
    "    phone['MultipleLines'] = phone['MultipleLines'].astype('category')\n",
    "\n",
    "print(\"Tipos de datos corregidos en los otros DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8398f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 7: FUSIÓN DE LOS DATAFRAMES\n",
    "# ============================================================================================\n",
    "# Se fusionan los cuatro DataFrames en uno solo, 'df_merged', utilizando 'customerID' como la clave de unión.\n",
    "# Se utiliza un 'outer join' para asegurar que no se pierda información de ningún cliente,\n",
    "# incluso si no tienen registros en todas las tablas.\n",
    "\n",
    "df_merged = contract.merge(internet, on='customerID', how='outer') \\\n",
    "                    .merge(personal, on='customerID', how='outer') \\\n",
    "                    .merge(phone, on='customerID', how='outer')\n",
    "\n",
    "# Se normalizan los nombres de las columnas a minúsculas para facilitar el acceso.\n",
    "df_merged.columns = df_merged.columns.str.lower()\n",
    "\n",
    "print(\"DataFrame fusionado. Dimensiones:\", df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 8: INGENIERÍA DE CARACTERÍSTICAS (CHURN Y TENURE)\n",
    "# ============================================================================================\n",
    "# Se crean dos nuevas columnas clave para el análisis.\n",
    "\n",
    "# 1. 'churn': Nuestra variable objetivo.\n",
    "#    - Si 'enddate' es nulo (NaT), el cliente está activo (churn = 0).\n",
    "#    - Si 'enddate' tiene una fecha, el cliente ha cancelado (churn = 1).\n",
    "df_merged['churn'] = np.where(df_merged['enddate'].isna(), 0, 1)\n",
    "\n",
    "# 2. 'tenure_months': La antigüedad del cliente en meses.\n",
    "#    - Se calcula la diferencia entre 'enddate' y 'begindate'.\n",
    "#    - Para clientes activos, se usa la fecha actual como 'enddate' para el cálculo.\n",
    "#    - El resultado en días se convierte a meses (aproximadamente).\n",
    "df_merged['tenure_months'] = ((df_merged['enddate'].fillna(pd.Timestamp.now()) - df_merged['begindate']).dt.days // 30).astype('Int64')\n",
    "\n",
    "# Se guarda una copia del DataFrame limpio para futuras referencias.\n",
    "df_merged_clean = df_merged.copy()\n",
    "\n",
    "print(\"Columnas 'churn' y 'tenure_months' creadas.\")\n",
    "print(\"\\nConteo de la variable objetivo 'churn':\")\n",
    "display(df_merged_clean['churn'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f2445",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "En esta sección, visualizaremos los datos para extraer insights. El objetivo es entender las relaciones entre las diferentes características y la tasa de cancelación (`churn`).\n",
    "- **Distribuciones Numéricas:** Analizaremos la distribución de variables como `tenure_months` y `totalcharges`.\n",
    "- **Matriz de Correlación:** Identificaremos relaciones lineales entre las variables numéricas.\n",
    "- **Análisis de Churn:** Estudiaremos cómo la tasa de churn varía según diferentes categorías, como el tipo de contrato, el método de pago y los servicios adicionales contratados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 9: ESTADÍSTICAS DESCRIPTIVAS\n",
    "# ============================================================================================\n",
    "# Se generan estadísticas descriptivas para las variables numéricas y categóricas.\n",
    "# .describe(include='number') resume las métricas estadísticas (media, mediana, etc.) para columnas numéricas.\n",
    "# .describe(include=['category','object']) resume las métricas para columnas categóricas (conteo, valores únicos, etc.).\n",
    "\n",
    "print(\"Estadísticas de variables numéricas:\")\n",
    "display(df_merged_clean.describe(include='number').T.round(2))\n",
    "\n",
    "print(\"\\nEstadísticas de variables categóricas:\")\n",
    "display(df_merged_clean.describe(include=['category','object']).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ba0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 10: VISUALIZACIÓN DE VARIABLES NUMÉRICAS\n",
    "# ============================================================================================\n",
    "# Se crean histogramas para visualizar la distribución de las variables numéricas.\n",
    "# Esto ayuda a entender la forma, el centro y la dispersión de cada característica.\n",
    "\n",
    "# Se seleccionan las columnas de tipo numérico.\n",
    "num_cols = df_merged_clean.select_dtypes(include=['int64','float64','Int64']).columns.tolist()\n",
    "# Se retiran columnas irrelevantes para la distribución, como 'churn' que es binaria.\n",
    "num_cols = [c for c in num_cols if c not in ['churn']]\n",
    "\n",
    "\n",
    "# Se crea una figura para los histogramas.\n",
    "plt.figure(figsize=(12, 8))\n",
    "df_merged_clean[num_cols].hist(bins=30, layout=(len(num_cols)//2, 2), figsize=(14, 8))\n",
    "plt.suptitle('Distribuciones de Variables Numéricas', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388aed2",
   "metadata": {},
   "source": [
    "<div style=\"font-size:12px;\">\n",
    "\n",
    "<h1 style=\"color:#1E90FF;\">Interpretación de las Distribuciones Numéricas</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<p>\n",
    "Los histogramas revelan patrones interesantes:\n",
    "</p>\n",
    "<ul>\n",
    "  <li><strong><code>tenure_months</code> (Antigüedad):</strong> La distribución es bimodal. Hay un gran grupo de clientes nuevos (baja antigüedad) y otro grupo considerable de clientes muy leales (alta antigüedad). Esto sugiere que si un cliente supera una fase inicial crítica, es probable que permanezca por mucho tiempo.</li>\n",
    "  <li><strong><code>monthlycharges</code> (Cargos Mensuales):</strong> La mayoría de los clientes se concentran en el rango de cargos más bajos (alrededor de $20-$30), probablemente correspondiendo a planes básicos. Hay una distribución más dispersa hacia cargos más altos, indicando una variedad de planes y servicios adicionales.</li>\n",
    "  <li><strong><code>totalcharges</code> (Cargos Totales):</strong> Esta variable muestra un fuerte sesgo a la derecha. Muchos clientes tienen cargos totales bajos, lo cual es consistente con la gran cantidad de clientes nuevos. A medida que la antigüedad aumenta, los cargos totales acumulados también lo hacen.</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 11: MATRIZ DE CORRELACIÓN\n",
    "# ============================================================================================\n",
    "# Se calcula y visualiza una matriz de correlación para las variables numéricas.\n",
    "# El mapa de calor (heatmap) muestra la fuerza y dirección de la relación lineal entre pares de variables.\n",
    "# Valores cercanos a 1 o -1 indican una correlación fuerte, mientras que valores cercanos a 0 indican una correlación débil.\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Se calcula la matriz de correlación.\n",
    "corr = df_merged_clean.select_dtypes(include=['number']).corr()\n",
    "# Se dibuja el mapa de calor.\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Matriz de Correlación de Variables Numéricas', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce890799",
   "metadata": {},
   "source": [
    "<div style=\"font-size:12px;\">\n",
    "\n",
    "<h1 style=\"color:#1E90FF;\">Interpretación de la Matriz de Correlación</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<p>\n",
    "El mapa de calor destaca las siguientes relaciones:\n",
    "</p>\n",
    "<ul>\n",
    "  <li><strong>Fuerte Correlación Positiva:</strong>\n",
    "    <ul>\n",
    "      <li><code>tenure_months</code> y <code>totalcharges</code> (0.83): Es una relación lógica. Cuanto más tiempo un cliente permanece en la compañía, más paga en total.</li>\n",
    "      <li><code>monthlycharges</code> y <code>totalcharges</code> (0.65): También esperado. Clientes con planes más caros acumulan cargos totales más rápidamente.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><strong>Correlación Negativa:</strong>\n",
    "    <ul>\n",
    "      <li><code>churn</code> y <code>tenure_months</code> (-0.35): Esta es una de las correlaciones más importantes para nuestro objetivo. Indica que <strong>a mayor antigüedad del cliente, menor es la probabilidad de que cancele el servicio</strong>. Los clientes nuevos son los más propensos a irse.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "    <li><strong>Correlación Positiva con Churn:</strong>\n",
    "    <ul>\n",
    "      <li><code>churn</code> y <code>monthlycharges</code> (0.19): Hay una leve tendencia a que los clientes con cargos mensuales más altos sean más propensos a cancelar. Esto podría deberse a una percepción de bajo valor por el precio pagado.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fe402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 12: ANÁLISIS UNIVARIADO DE LA VARIABLE OBJETIVO (CHURN)\n",
    "# ============================================================================================\n",
    "# Se analiza la distribución de la variable objetivo 'churn' para entender el balance de clases.\n",
    "# Un desbalance significativo (muchos más ejemplos de una clase que de otra) puede requerir técnicas especiales en el modelado.\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='churn', data=df_merged_clean, palette='Set1')\n",
    "plt.title('Distribución de la Variable Objetivo (Churn)', fontsize=14)\n",
    "plt.xlabel('Churn (0 = No, 1 = Sí)')\n",
    "plt.ylabel('Cantidad de Clientes')\n",
    "plt.show()\n",
    "\n",
    "# Se muestran los conteos y porcentajes exactos.\n",
    "print(\"Conteo y porcentaje por clase de 'churn':\")\n",
    "display(pd.DataFrame({\n",
    "    'count': df_merged_clean['churn'].value_counts(),\n",
    "    'pct': (df_merged_clean['churn'].value_counts(normalize=True) * 100).round(2)\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87398a71",
   "metadata": {},
   "source": [
    "<div style=\"font-size:12px;\">\n",
    "\n",
    "<h1 style=\"color:#1E90FF;\">Desbalance de Clases en la Variable Objetivo</h1>\n",
    "\n",
    "<p>\n",
    "Se observa un <strong style=\"color:#D0021B;\">desbalance de clases</strong> en la variable objetivo <strong>churn</strong>, donde la mayoría de los clientes \n",
    "<strong style=\"color:#228B22;\">no han abandonado</strong> el servicio.  \n",
    "Debido a esto, será necesario considerar <strong style=\"color:#F5A623;\">técnicas de balanceo de clases</strong> en etapas posteriores del proyecto \n",
    "para mejorar el rendimiento de los modelos de Machine Learning.\n",
    "</p>\n",
    "\n",
    "<h1 style=\"color:#32CD32;\">Cuándo Usar Cada Técnica de Balanceo</h1>\n",
    "\n",
    "<h2 style=\"color:#8A2BE2;\">Sobremuestreo (Oversampling)</h2>\n",
    "<ul>\n",
    "  <li>Útil cuando la <strong style=\"color:#8A2BE2;\">clase minoritaria tiene muy pocos ejemplos</strong>.</li>\n",
    "  <li>Genera ejemplos duplicados o sintéticos para mejorar el aprendizaje del modelo.</li>\n",
    "  <li>Técnicas como <strong style=\"color:#F8E71C;\">SMOTE</strong> ayudan a crear nuevos registros sin duplicación literal.</li>\n",
    "  <li>Puede aumentar el riesgo de <strong style=\"color:#8A2BE2;\">sobreajuste</strong> si no se usa correctamente.</li>\n",
    "</ul>\n",
    "\n",
    "<h2 style=\"color:#D0021B;\">Submuestreo (Undersampling)</h2>\n",
    "<ul>\n",
    "  <li>Recomendado cuando la <strong style=\"color:#D0021B;\">clase mayoritaria tiene demasiados ejemplos</strong>.</li>\n",
    "  <li>Reduce el tamaño del dataset eliminando parte de la clase dominante.</li>\n",
    "  <li>Acelera el entrenamiento de los modelos.</li>\n",
    "  <li>Puede provocar pérdida de información valiosa de la clase mayoritaria.</li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468fb0b",
   "metadata": {},
   "source": [
    "## 4. Preparación de Datos para Modelado\n",
    "\n",
    "Antes de entrenar los modelos, debemos transformar nuestros datos a un formato puramente numérico que los algoritmos de Machine Learning puedan procesar.\n",
    "- **Selección de Características:** Se definirán las variables predictoras (X) y la variable objetivo (y).\n",
    "- **División de Datos:** El conjunto de datos se dividirá en un conjunto de entrenamiento (para entrenar el modelo) y un conjunto de prueba (para evaluarlo de forma imparcial).\n",
    "- **Pipelines de Preprocesamiento:** Se crearán `Pipelines` para automatizar la transformación de los datos. Las variables numéricas serán escaladas (para que tengan una media de 0 y desviación estándar de 1) y las variables categóricas serán convertidas a formato numérico mediante `One-Hot Encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 13: IMPORTACIÓN DE LIBRERÍAS DE MACHINE LEARNING\n",
    "# ============================================================================================\n",
    "# Se importan las clases y funciones necesarias de Scikit-learn y otras librerías de ML.\n",
    "\n",
    "# Para dividir los datos en conjuntos de entrenamiento y prueba.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Para escalar características numéricas y codificar las categóricas.\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# Para imputar valores faltantes.\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Para aplicar diferentes transformaciones a diferentes columnas.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Para encadenar pasos de preprocesamiento y modelado.\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Para manejar el desbalance de clases con SMOTE.\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# Modelos de clasificación que se van a entrenar.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# Métricas para evaluar el rendimiento de los modelos.\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Se ocultan advertencias para una salida más limpia.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 14: DEFINICIÓN DE VARIABLES Y DIVISIÓN DE DATOS\n",
    "# ============================================================================================\n",
    "# Se definen la variable objetivo (y) y las variables predictoras (X).\n",
    "\n",
    "# 'y' es la columna 'churn', que queremos predecir.\n",
    "y = df_merged_clean[\"churn\"]\n",
    "# 'X' contiene todas las demás columnas relevantes que se usarán para la predicción.\n",
    "# Se eliminan columnas no predictivas como IDs y fechas.\n",
    "X = df_merged_clean.drop(columns=[\"churn\", \"customerid\", \"begindate\", \"enddate\"])\n",
    "\n",
    "# Se identifican automáticamente las columnas numéricas y categóricas en X.\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64', 'Int64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Se dividen los datos en un 80% para entrenamiento y un 20% para prueba.\n",
    "# 'stratify=y' asegura que la proporción de churn sea la misma en ambos conjuntos,\n",
    "# lo cual es importante en datasets desbalanceados.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Datos divididos en conjuntos de entrenamiento y prueba.\")\n",
    "print(\"Tamaño de X_train:\", X_train.shape)\n",
    "print(\"Tamaño de X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca74349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 15: CREACIÓN DE PIPELINES DE PREPROCESAMIENTO\n",
    "# ============================================================================================\n",
    "# Se definen los pasos de preprocesamiento para cada tipo de variable.\n",
    "\n",
    "# Pipeline para variables numéricas:\n",
    "# 1. Imputer: Rellena cualquier valor faltante con la mediana de la columna.\n",
    "# 2. Scaler: Estandariza las características (media 0, varianza 1).\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para variables categóricas:\n",
    "# 1. Imputer: Rellena cualquier valor faltante con el valor más frecuente (moda).\n",
    "# 2. OneHotEncoder: Convierte cada categoría en una nueva columna binaria (0 o 1).\n",
    "#    'handle_unknown='ignore'' evita errores si aparecen nuevas categorías en los datos de prueba.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "# Se utiliza ColumnTransformer para aplicar los pipelines correctos a las columnas correctas.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Mantiene las columnas no especificadas (si las hubiera)\n",
    ")\n",
    "\n",
    "print(\"Pipelines de preprocesamiento creados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e5c34",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento y Evaluación de Modelos\n",
    "\n",
    "En esta fase final, se entrenarán varios algoritmos de clasificación. Para cada uno, se seguirá el mismo proceso:\n",
    "1.  **Creación de un Pipeline Completo:** Se combinará el preprocesamiento, el sobremuestreo SMOTENC (para manejar el desbalance de clases) y el modelo en un único `Pipeline`.\n",
    "2.  **Entrenamiento:** El pipeline se entrenará con los datos de entrenamiento (`X_train`, `y_train`).\n",
    "3.  **Evaluación:** El modelo entrenado se usará para hacer predicciones sobre los datos de prueba (`X_test`), y su rendimiento se medirá con métricas como `Precision`, `Recall`, `F1-score` y una `Matriz de Confusión`.\n",
    "\n",
    "Se evaluarán los siguientes modelos:\n",
    "- **Dummy Classifier:** Un modelo de referencia que siempre predice la clase más frecuente.\n",
    "- **Logistic Regression:** Un modelo lineal simple y robusto.\n",
    "- **Decision Tree:** Un modelo basado en reglas, fácil de interpretar.\n",
    "- **Random Forest:** Un modelo de ensamble que combina múltiples árboles de decisión para mejorar la robustez.\n",
    "- **XGBoost:** Un modelo de gradient boosting avanzado, conocido por su alto rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8763e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 16: DEFINICIÓN DE LOS MODELOS A EVALUAR\n",
    "# ============================================================================================\n",
    "# Se crea un diccionario que contiene los modelos que se van a entrenar y evaluar.\n",
    "# Los hiperparámetros de cada modelo han sido pre-seleccionados.\n",
    "# 'class_weight='balanced'' o 'scale_pos_weight' son técnicas para que el modelo\n",
    "# preste más atención a la clase minoritaria (churn).\n",
    "\n",
    "models = {\n",
    "    \"Dummy Baseline\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, class_weight='balanced', random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=7, class_weight='balanced', random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=5, scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(), use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Diccionario para almacenar los resultados de las métricas de cada modelo.\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 17: BUCLE DE ENTRENAMIENTO Y EVALUACIÓN\n",
    "# ============================================================================================\n",
    "# Se itera sobre cada modelo definido en el diccionario 'models'.\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} ENTRENANDO Y EVALUANDO: {name.upper()} {'='*20}\")\n",
    "\n",
    "    # Se crea un pipeline completo que integra preprocesamiento, SMOTENC y el modelo.\n",
    "    # SMOTENC (Synthetic Minority Over-sampling Technique for Nominal and Continuous)\n",
    "    # genera ejemplos sintéticos de la clase minoritaria para balancear el dataset de entrenamiento.\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocess', preprocessor),\n",
    "        ('smote', SMOTENC(\n",
    "            categorical_features=[X_train.columns.get_loc(col) for col in categorical_features],\n",
    "            random_state=42\n",
    "        )),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Se entrena el pipeline completo con los datos de entrenamiento.\n",
    "    # SMOTE solo se aplica a los datos de entrenamiento para evitar fuga de datos (data leakage).\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Se realizan predicciones sobre el conjunto de prueba (que no ha sido visto por el modelo).\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Se genera y muestra la matriz de confusión.\n",
    "    # La matriz muestra los aciertos y errores del modelo (Verdaderos Positivos, Falsos Negativos, etc.).\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Churn\", \"Churn\"])\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(f\"Matriz de Confusión - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Se imprime el reporte de clasificación, que incluye Precision, Recall y F1-score.\n",
    "    print(f\"Reporte de Clasificación - {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Se guardan los resultados para una comparación final.\n",
    "    results[name] = classification_report(y_test, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51623313",
   "metadata": {},
   "source": [
    "## 6. Conclusiones y Selección del Mejor Modelo\n",
    "\n",
    "Tras evaluar todos los modelos, se comparan sus resultados para seleccionar el más adecuado para el problema. La métrica principal para la selección será el **F1-score para la clase \"Churn\" (1)**, ya que proporciona un buen equilibrio entre `Precision` (evitar falsos positivos) y `Recall` (identificar a todos los clientes que realmente harán churn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af01f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# CELDA 18: COMPARACIÓN FINAL DE MODELOS\n",
    "# ============================================================================================\n",
    "# Se extrae el F1-score para la clase 1 (Churn) de cada modelo para una comparación directa.\n",
    "\n",
    "f1_scores_churn = {name: results[name]['1']['f1-score'] for name in results}\n",
    "\n",
    "# Se crea un DataFrame para visualizar los F1-scores de manera ordenada.\n",
    "f1_df = pd.DataFrame.from_dict(f1_scores_churn, orient='index', columns=['F1-Score (Churn)'])\n",
    "f1_df = f1_df.sort_values(by='F1-Score (Churn)', ascending=False)\n",
    "\n",
    "print(\"Comparación del F1-Score para la clase 'Churn' (1):\")\n",
    "display(f1_df)\n",
    "\n",
    "# Se crea un gráfico de barras para visualizar la comparación.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=f1_df.index, y=f1_df['F1-Score (Churn)'])\n",
    "plt.title('Comparación de Modelos: F1-Score para Churn', fontsize=16)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195364ad",
   "metadata": {},
   "source": [
    "### **Conclusión Final del Proyecto**\n",
    "\n",
    "El análisis y modelado de datos han revelado información valiosa sobre los factores que impulsan la cancelación de clientes y han permitido construir un modelo predictivo eficaz.\n",
    "\n",
    "**Modelo Seleccionado:**\n",
    "\n",
    "El modelo **Random Forest** se destaca como la mejor opción. Aunque otros modelos como Logistic Regression y XGBoost muestran métricas ligeramente superiores, el Random Forest ofrece un excelente equilibrio entre rendimiento y robustez, con un **F1-score de 0.61** para la clase Churn. Es menos propenso al sobreajuste que un solo árbol de decisión y más interpretable que XGBoost.\n",
    "\n",
    "**Recomendaciones Estratégicas:**\n",
    "\n",
    "1.  **Enfocarse en Clientes con Contratos Mensuales:** Dado que los clientes con contratos `Month-to-month` tienen la tasa de churn más alta, se recomienda ofrecerles incentivos para migrar a contratos anuales o de dos años (ej. descuentos, beneficios adicionales).\n",
    "\n",
    "2.  **Promover Servicios de Retención:** Los servicios de `TechSupport` y `OnlineSecurity` demostraron ser cruciales para la retención. Se deberían promover activamente entre los clientes que no los tienen, especialmente si el modelo los marca como de alto riesgo.\n",
    "\n",
    "3.  **Revisar el Método de Pago \"Electronic Check\":** Este método de pago está fuertemente correlacionado con el churn. Sería valioso investigar por qué (ej. problemas de usabilidad, fallos en los pagos) y fomentar el uso de métodos más estables como el débito automático o la tarjeta de crédito.\n",
    "\n",
    "**Próximos Pasos:**\n",
    "\n",
    "- **Optimización de Hiperparámetros:** Realizar una búsqueda más exhaustiva de los mejores hiperparámetros para el modelo Random Forest podría mejorar aún más su rendimiento.\n",
    "- **Análisis de Características:** Utilizar las `feature_importances_` del modelo Random Forest para entender en mayor detalle qué variables son las más influyentes en la predicción.\n",
    "- **Implementación:** Desarrollar un plan para desplegar el modelo en un entorno de producción, permitiendo que las predicciones se generen de forma regular y se integren en las campañas de retención del negocio."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
